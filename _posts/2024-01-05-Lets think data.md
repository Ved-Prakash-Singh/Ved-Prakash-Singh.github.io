---
layout: post
title: Lets think data
---
Have you ever wondered, how much data is there in the world? At what accelated velocity, these data are being created? How much of these data are original and how much are duplicate?

Yes, you are right, it is very difficult or near impossible to get these numbers. In this post, I will talk about some of the lower bond approximated numbers based upon certain research organizations. 

Data are in various forms, they are online data, they are in term of physical data like book, paper documents, they are also in stand alone disk storages etc. In this post, I will be focussed around online data, which can be considered as lower bound number.

As per an estimate from International Data Corporation (IDC), the overall online datasize was around 64 zettabytes in 2020. IDC predicts that the Global Datasphere will grow from 33 Zettabytes in 2018 to 175 Zettabytes by 2025.
One zettabyte is approximately equal to 1 billion terabytes. 
If you were able to store the entire Global Datasphere on DVDs, then you would have a stack of DVDs that could get you to the moon 23 times or circle Earth 222 times.
If you could download the entire 2025 Global Datasphere at an average of 25 Mb/s, today's average connection speed across the United States, then it would take one person 1.8 billion years to do it, or if every person in the world could help and never rest, then you could get it done in 81 days
There is 90% replicated data in the global datasphere, with only 10% being unique data. 

Reference: <br>
https://www.seagate.com/files/www-content/our-story/trends/files/idc-seagate-dataage-whitepaper.pdf
https://www.visualcapitalist.com/wp-content/uploads/2019/04/data-generated-each-day-wide.html
https://www.statista.com/statistics/1185888/worldwide-global-datasphere-unique-replicated-data/

**what fraction of these data are labelled data? How labelling is constraint to use these data for AI algorithms? What are the techniques which researchers have came up with to utilize unlabelled data? 
